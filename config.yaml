seed_everything: 11
trainer:
  logger:
    - class_path: pytorch_lightning.loggers.WandbLogger
      init_args:
        project: stock-tweets
        log_model: false
  max_epochs: 15
model:
  optim: AdamW
  lr: 0.001
  transformer_model: &hug_model nlptown/bert-base-multilingual-uncased-sentiment # (5 out) 12-layer, 768-hidden, 12-heads, 168M parameters
  # transformer_model: cardiffnlp/twitter-roberta-base-sentiment # (3 out) 12-layer, 768-hidden, 12-heads, 125M parameters
  # transformer_model: albert-base-v2 # (2 out) 12 repeating layers, 128 embedding, 768-hidden, 12-heads, 11M parameters
  transformer_out: 5
  freeze_transformer: False
  hidden_dim: 0
  classify_threshold_down: &thres_down -0.005
  classify_threshold_up: &thres_up +0.007 # +0.0055
  attention_input: both # followers # sentiment
data:
  batch_size: 1
  train_size: 0.8
  val_size: 0.1
  transformer_model: *hug_model
  classify_threshold_down: *thres_down
  classify_threshold_up: *thres_up
  tweet_max_len: 50 # 100 # TODO revisit
  min_followers: 100  # TODO try multiple values
  min_tweets_day: 5
  time_lag: 1
  tweet_path: data/tweet/raw
  price_path: data/price/preprocessed
  num_workers: 1 # TODO set back to 32
