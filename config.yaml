batch_size:
  desc: Batch size to use for train/val/test loading
  value: 1
optim:
  desc: Name of torch.optim.* optimizer to use
  value: Adam
lr:
  desc: Learning rate to use for the optimiser
  value: 0.001
epochs:
  desc: Number of epochs
  value: 15
transformer_model:
  desc: Which Hugging Face transformer model to use
  value: nlptown/bert-base-multilingual-uncased-sentiment # (5 out) 12-layer, 768-hidden, 12-heads, 168M parameters
# value: cardiffnlp/twitter-roberta-base-sentiment # (3 out) 12-layer, 768-hidden, 12-heads, 125M parameters
# value: albert-base-v2 # (2 out) 12 repeating layers, 128 embedding, 768-hidden, 12-heads, 11M parameters
transformer_out:
  desc: The number of output values of the transformer
  value: 5
loss_type:
  desc: Wheter to train regression task (MSELoss) or classification task (BCEWithLogitsLoss)
# value: regression
  value: classification
freeze_transformer:
  desc: If true, freezes transformer weights, or fine-tune them otherwise
  value: False
train_size:
  desc: Portion of data to use for training
  value: 0.8
val_size:
  desc: Portion of data to use for validation
  value: 0.1
hidden_dim:
  desc: Hidden dimension to use for classification layer. If 0, no hidden layer is used
  value: 0
device:
  desc: Which device to use for training (~ for auto)
  value: ~
classify_threshold_down:
  desc: Threshold for when a course is classified as going DOWN
  value: -0.005
classify_threshold_up:
  desc: Threshold for when a course is classified as going UP
  value: +0.007 # +0.0055
min_tweets_day:
  desc: Minimum number of tweets about a stock on one day for a movement to be considered as a sample
  value: 5
time_lag:
  desc: Number of days between tweets and supposed market reaction
  value: 1
attention_input:
  desc: which inputs to use for the attention ('followers', 'sentiment' or 'both')
  value: both